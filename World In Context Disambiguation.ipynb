{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TO_BE_DELIVERED.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1GwvVJpQjWAbc6EP7e7jqt4nSeP5O-wbd","authorship_tag":"ABX9TyM+4m6SyxIFu9h3zBv6mFcU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_ymLuhTqTVci"},"source":["\n","\n","# Setting up environment\n","\n","#### here we import and prepare the working environment.\n","\n"]},{"cell_type":"code","metadata":{"id":"6b1U4k-iVmvr"},"source":["# general  \n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from collections import Counter, defaultdict\n","from tqdm.notebook import tqdm\n","from typing import *\n","\n","# torch\n","import torch\n","\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import SGD\n","from torch.optim import Adam\n","\n","# auxiliar\n","import os\n","import json\n","import pickle\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8u-iz7B7hOCU"},"source":["! rm -rf sample_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vk8XN66TWfht","executionInfo":{"status":"ok","timestamp":1619426637278,"user_tz":-120,"elapsed":1040,"user":{"displayName":"lorenzo mandelli","photoUrl":"","userId":"01769209783218279689"}},"outputId":"3b42738a-c21f-482f-c343-461abd040520"},"source":["%cd '/content/drive/MyDrive/NLP/nlp2021-hw1-main/data'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/NLP/nlp2021-hw1-main/data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6WbdusVliUPv"},"source":["\n","\n","# One time run\n","\n","#### here we download embeddings such as glove.\n","\n"]},{"cell_type":"code","metadata":{"id":"btRSsqRmLx10"},"source":["#download and unzip word embedding , I used GloVe\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove*.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-nOFVvWiqxb"},"source":["\n","# Create dictioray word_vectors and word_index\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"mDTwkDTSFXxE"},"source":["# build vocabulary and associate heach word to his embedding, here I use a 300-dim embedding\n","word_vectors = dict()\n","words_limit = 400_000\n","with open('/content/drive/MyDrive/NLP/nlp2021-hw1-main/data/glove.6B.300d.txt',encoding='utf8') as f:\n","\n","    for i, line in tqdm(enumerate(f), total=words_limit):\n","      \n","        if i == words_limit:\n","            break\n","\n","        word, *vector = line.strip().split(' ')\n","        vector = torch.tensor([float(c) for c in vector])\n","        \n","        word_vectors[word] = vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aAwvch9SZCkZ"},"source":["# save word_vectors as a pickle file\n","with open('words_vectors.pickle', 'wb') as handle:\n","    pickle.dump(word_vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcmTkpdK8X2z"},"source":["# pair each word in the vocabulary to an index which ( I used the embedding proposed during the lectures )\n","word_index = dict()\n","vectors_store = []\n","\n","# pad token, index = 0\n","vectors_store.append(torch.rand(300))\n","\n","# unk token, index = 1\n","vectors_store.append(torch.rand(300))\n","\n","for word, vector in word_vectors.items():\n","\n","    word_index[word]=len(vectors_store)\n","    vectors_store.append(vector)\n","\n","word_index = defaultdict(lambda: 1, word_index)  # default dict returns 1 (unk token) when unknown word\n","vectors_store = torch.stack(vectors_store)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_m4RJKqZB2V"},"source":["# save word_index dict as a pickle file\n","with open('words_index.pickle', 'wb') as handle:\n","    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tYLJ96b1i8lt"},"source":["\n","\n","# Functions\n","\n","#### Here we define general function we need for preprocessing and for LSTM and MLP training and testing.\n","\n"]},{"cell_type":"code","metadata":{"id":"KT3RERqaJv_8"},"source":["# aggregation function which uses simple mean of vectors associated to the words composing the sentences\n","\n","def sentence2vector(sentence: str) -> Optional[torch.Tensor]:\n","    sentences_word_vector = [word_vectors[w] for w in sentence.split(' ') if w in word_vectors]\n","    \n","    if len(sentences_word_vector) == 0:\n","        return None\n","\n","    sentences_word_vector = torch.stack(sentences_word_vector)  \n","    return torch.mean(sentences_word_vector, dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1KVkKbFkjGvi"},"source":["# weighted sum of vectors associated with the words composing the sentence\n","# the weights depend on the frequency of the word in our dataset : \n","# words with few occurencies contribute more than words which occur more frequently \n","# we use the words_organized dictionary in order to distinguish such words\n","\n","def sentence2vector0(sentence: str) -> Optional[torch.Tensor]:\n","    sentences_word_vector = []\n","    for w in sentence.split(' '):\n","       if (w in word_vectors):\n","\n","         if (w in words_organized['few_occ']):  sentences_word_vector.append(word_vectors[w]*0.75)\n","         elif (w in words_organized['mean_occ']): sentences_word_vector.append(word_vectors[w]*0.6)\n","         else: sentences_word_vector.append(word_vectors[w]*0.45) \n","    \n","    if len(sentences_word_vector) == 0:\n","        return None\n","\n","    sentences_word_vector = torch.stack(sentences_word_vector)  \n","    return torch.sum(sentences_word_vector,dim=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORkpAWg88TmY"},"source":["# pairs words in the sentence with indices from 0 to size of vocabulary\n","\n","def sentence2indices(sentence: str) -> torch.Tensor:\n","    return torch.tensor([word_index[word] for word in sentence.split(' ')], dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVwon_TD8Qmq"},"source":["# build sequences taking in account sequence length, I used the solution proposed during the lectures\n","\n","def rnn_collate_fn(\n","    data_elements: List[Tuple[torch.Tensor, torch.Tensor]]\n",") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","    X = [de[0] for de in data_elements] \n","\n","    X_lengths = torch.tensor([x.size(0) for x in X], dtype=torch.int)\n","\n","    X = torch.nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0) \n","\n","    y = [de[1] for de in data_elements]\n","    y = torch.tensor(y)\n","\n","    return X, X_lengths, y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fS-qZBGgG5j7"},"source":["# remove punctuation and some special character , I decided to avoid using tokenizers and build a function from scratch\n","\n","def  rm_punctuation(sentence:str) -> str:\n","  result = sentence.replace('\"','').replace(',','').replace(':','').replace('.','').replace(';','').replace('—','').replace('”','').replace('“','').replace('-','')\n","  result = result.replace('(','').replace(')','').replace('[','').replace(']','').replace('{','').replace('}','').replace('+','').replace('’','').replace('/','')\n","  return result\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CRhQALLFlSA"},"source":["# remove a list of words from a sentence\n"," \n","def rm_words(sentence:str,rm_words:list) -> str:\n","  resultwords  = [word for word in sentence.split() if word not in rm_words]\n","  result = ' '.join(resultwords)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyQUrH3FQQcU"},"source":["# replace the the portion of the sentence associated with the lemma with the lemma itself ( replace pluarals , inflections with the plain lemma)\n","\n","def search_replace_word(sentence:str,replace_word:str,start:int,end:int) -> str:\n","  word_tobe_replaced = ''\n","  for i in range(start,end):\n","    word_tobe_replaced += sentence.strip()[i]\n","  result = sentence.replace(word_tobe_replaced,replace_word)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnTd5EOQO9iX"},"source":["# remove numbers from the sentence\n","\n","def rm_numbers(sentence : str) -> str:\n","  result_words = []\n","  for word in sentence.split():\n","    try:\n","      if (isinstance(int(word),int)):\n","        sentence = sentence.replace(word,'')\n","    except:\n","      continue\n","\n","  return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5elAHhyyJ7go","executionInfo":{"status":"ok","timestamp":1619977040817,"user_tz":-120,"elapsed":2642,"user":{"displayName":"lorenzo mandelli","photoUrl":"","userId":"01769209783218279689"}}},"source":["# eliminate some words randomly according to theyr frequency\n","\n","def refine_sentence(sentence:str,few:list,mean:list,more:list) -> str:\n","  resultwords = []\n","  for word in sentence.split():\n","    if (word in words_organized['few_occ']): \n","      if (random.random() < 0.5): \n","        resultwords.append(word)\n","    if (word in words_organized['mean_occ']):\n","      if (random.random() < 0.75 ):\n","        resultwords.append(word)\n","    if (word in words_organized['more_occ']):\n","      if (random.random() < 0.9) :\n","        resultwords.append(word)\n","\n","  result = ' '.join(resultwords)\n","  return result"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gWIaNIuPjOC"},"source":["# build a dictionary which contains as elements the lists of word which occurs with different frequence\n","\n","def organize_words(path:str) -> Tuple[List[str]]:\n","\n","  word_freq = {}\n","  words_organized = {}\n","  words = []\n","  lemmas = []\n","  few_occ = []\n","  mean_occ = []\n","  more_occ = []\n","  most_occ = []\n","\n","\n","  with open(path) as f:\n","      for line in f:\n","\n","        dict_from_line  = eval(line)\n","\n","        sentence1 = dict_from_line['sentence1'].lower()\n","        sentence2 = dict_from_line['sentence2'].lower()\n","        sentence1 = rm_punctuation(sentence1)\n","        sentence2 = rm_punctuation(sentence2)\n","        lemma = dict_from_line['lemma']\n","\n","        if (lemma not in lemmas): lemmas.append(lemma)\n","\n","        for word in sentence1.split():\n","          if (word not in words): \n","            words.append(word)\n","            word_freq[word] = 1    \n","          else:\n","            word_freq[word] += 1\n","\n","        for word in sentence2.split():\n","          if (word not in words): \n","            words.append(word)\n","            word_freq[word] = 1    \n","          else:\n","            word_freq[word] += 1\n","\n","      for key,value in word_freq.items():\n","        if (key not in lemmas):\n","          if ( 0 < value < 2 ): few_occ.append(key)\n","          if ( 2 <= value < 250 ):mean_occ.append(key)\n","          if ( 250 <= value < 500 ):more_occ.append(key)      \n","          if ( 500 <= value ):most_occ.append(key)  \n","\n","      words_organized['few_occ'] = few_occ   \n","      words_organized['mean_occ'] = mean_occ    \n","      words_organized['more_occ'] = more_occ    \n","      words_organized['most_occ'] = most_occ    \n"," \n","\n","  return words_organized\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ssbhBSJ_s59K"},"source":["\n","\n","## Loops for RNN\n","\n","#### Here we define the training and testing loops for RNN (LSTM) models.\n","\n"]},{"cell_type":"code","metadata":{"id":"0jK1aZkFiZ1z"},"source":["# loop I use for evaluating the model on the train dataset (LSTM)\n","\n","def train_acc(model: nn.Module):\n","  \n","  correct_pred = 0\n","  my_pred = 0    \n","  num_sample = 0  \n","  for data in train_dataloader:\n","      input, length , label = data\n","      label = label.long()\n","      batch_out = model(input,length,label)\n","      optimizer.zero_grad()\n","      try:\n","          for i in range(32):\n","            if (batch_out['pred'][i][0] > batch_out['pred'][i][1]):\n","              my_pred = torch.Tensor([0])\n","            else :\n","              my_pred = torch.Tensor([1])\n","            if (label[i] == my_pred):\n","              correct_pred += 1\n","            num_sample += 1\n","      except:\n","        continue\n","\n","  loss = batch_out['loss']\n","  acc = correct_pred/num_sample\n","  return acc , loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHS1-5NGc69t"},"source":["# loop I use for evaluating the model on the dev dataset (LSTM)\n","\n","def test_acc(model: nn.Module):\n","  \n","  correct_pred = 0\n","  my_pred = 0   \n","  num_sample = 0\n","  for data in test_dataloader:\n","      input, length , label = data\n","      label = label.long()\n","      optimizer.zero_grad()\n","      batch_out = model(input,length,label)\n","      try:\n","          for i in range(32):\n","            if (batch_out['pred'][i][0] > batch_out['pred'][i][1]):\n","              my_pred = torch.Tensor([0])\n","            else :\n","              my_pred = torch.Tensor([1])\n","            if (label[i] == my_pred):\n","              correct_pred += 1\n","            num_sample += 1\n","      except:\n","        continue\n","\n","  loss = batch_out['loss']\n","  acc = correct_pred/num_sample\n","  return acc , loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0H9zj7GapIVN"},"source":["# loop I use for training the model on the train dataset (LSTM)\n","# we save only the best model according to val_acc computed on the dev dataset and plot the losses and accuracies behavoiurs\n","\n","def training_loop(model: nn.Module, optimizer: torch.optim.Optimizer, epochs: int, epoch:int = 0):\n","    \n","    if (epoch == 0): \n","      max_acc = 0\n","      history_acc_train = []\n","      history_acc_val = []\n","      history_loss_train = []\n","      history_loss_val = []\n","\n","    epoch += 1\n","\n","    for epoch in range(epochs):\n","\n","        progress_bar = tqdm()\n","\n","        for data in train_dataloader:\n","\n","            input ,length, label = data\n","            label = label.long()\n","            optimizer.zero_grad()\n","            batch_out = model(input,length,label)\n","            loss = batch_out['loss']\n","            loss.backward()\n","            optimizer.step()\n","\n","            progress_bar.update()\n","\n","        t_acc , t_loss = train_acc(model)\n","        v_acc , v_loss = test_acc(model)\n","      \n","        progress_bar.set_postfix(epoch=epoch, train_loss= t_loss ,  train_acc = t_acc , val_loss = v_loss , val_acc  =  v_acc )\n","\n","        if (v_acc> max_acc):\n","          print('saving...')\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/NLP/model_state_dict.pt')\n","          max_acc= v_acc\n","\n","        history_acc_train.append(t_acc)\n","        history_acc_val.append(v_acc)\n","        history_loss_train.append(t_loss)\n","        history_loss_val.append(v_loss)\n","\n","        progress_bar.close()\n","\n","    history_acc = [history_acc_train , history_acc_val]\n","    history_loss = [history_loss_train, history_loss_val]\n","    for step in history_acc:\n","      plt.plot(step)\n","      plt.xlabel('Epochs')\n","      plt.ylabel('Accuracy')\n","    plt.show()\n","    for step in history_loss:\n","      plt.plot(step)\n","      plt.xlabel('Epochs')\n","      plt.ylabel('Loss')\n","    plt.show()\n","    \n","      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fIj2_BK2tJgh"},"source":["\n","\n","## Loops for MLP\n","\n","#### Here we define the training and testing loops for MLP model.\n","\n"]},{"cell_type":"code","metadata":{"id":"YUARHXmKsjXP"},"source":["# loop I use for evaluating the model on the train dataset (MLP)\n","\n","def train_acc(model: nn.Module):\n","  \n","  correct_pred = 0\n","  my_pred = 0    \n","  num_sample = 0  \n","  for data in train_dataloader:\n","      input , label = data\n","      optimizer.zero_grad()\n","      batch_out = model(input,label)\n","      try:\n","        for i in range(32):\n","          if (batch_out['pred'][i] < 0.5):\n","           my_pred = torch.Tensor([0])\n","          else :\n","            my_pred = torch.Tensor([1])\n","          if (label[i] == my_pred):\n","            correct_pred += 1\n","          num_sample += 1\n","      except:\n","        continue\n","\n","  loss = batch_out['loss']\n","  acc = correct_pred/num_sample\n","  return acc , loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDTD6yeosjXQ"},"source":["# loop I use for evaluating the model on the dev dataset (MLP)\n","\n","def test_acc(model: nn.Module):\n","  \n","  correct_pred = 0\n","  my_pred = 0   \n","  num_sample = 0\n","  for data in test_dataloader:\n","      input , label = data\n","      optimizer.zero_grad()\n","      batch_out = model(input,label)\n","      try:\n","        for i in range(32):\n","          if (batch_out['pred'][i] < 0.5):\n","            my_pred = torch.Tensor([0])\n","          else :\n","            my_pred = torch.Tensor([1])\n","          if (label[i] == my_pred):\n","            correct_pred += 1\n","          num_sample += 1\n","      except:\n","        continue\n","\n","  loss = batch_out['loss']\n","  acc = correct_pred/num_sample\n","  return acc , loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zl19S0TrsjXR"},"source":["# loop I use for training the model on the train dataset (MLP)\n","# we save only the best model according to val_acc computed on the dev dataset and plot the losses and accuracies behavoiurs\n","\n","def training_loop(model: nn.Module, optimizer: torch.optim.Optimizer, epochs: int):\n","    \n","\n","    max_acc = 0\n","    history_acc_train = []\n","    history_acc_val = []\n","    history_loss_train = []\n","    history_loss_val = []\n","\n","    for epoch in range(epochs):\n","\n","        progress_bar = tqdm()\n","\n","        for data in train_dataloader:\n","\n","            input , label = data\n","            optimizer.zero_grad()\n","            batch_out = model(input,label)\n","            loss = batch_out['loss']\n","            loss.backward()\n","            optimizer.step()\n","\n","            progress_bar.update()\n","\n","        t_acc , t_loss = train_acc(model)\n","        v_acc , v_loss = test_acc(model)\n","      \n","        progress_bar.set_postfix(epoch=epoch, train_loss = t_loss , val_loss = v_loss,train_acc = t_acc , val_acc = v_acc)\n","\n","        if (v_acc> max_acc):\n","          print('saving...')\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/NLP/model_0_hw' )\n","          max_acc = v_acc\n","\n","        history_acc_train.append(t_acc)\n","        history_acc_val.append(v_acc)\n","        history_loss_train.append(t_loss)\n","        history_loss_val.append(v_loss)\n","\n","        progress_bar.close()\n","\n","    history_acc = [history_acc_train , history_acc_val]\n","    history_loss = [history_loss_train, history_loss_val]\n","    for step in history_acc:\n","      plt.plot(step)\n","      plt.xlabel('Epochs')\n","      plt.ylabel('Accuracy')\n","    plt.show()\n","    plt.close()\n","\n","    for step in history_loss:\n","      plt.plot(step)\n","      plt.xlabel('Epochs')\n","      plt.ylabel('Loss')\n","    plt.show()\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRXSoArRximM"},"source":["\n","\n","## Experiment training loop \n","\n","#### Here we define the training loops for the experiment.\n","\n"]},{"cell_type":"code","metadata":{"id":"kA2uMT6PxZmI"},"source":["def training_loop(model: nn.Module, optimizer: torch.optim.Optimizer, epochs: int, descriptor: str):\n","    \n","\n","    max_acc = 0\n","    history_acc_train = []\n","    history_acc_val = []\n","    history_loss_train = []\n","    history_loss_val = []\n","\n","    for epoch in range(epochs):\n","\n","        progress_bar = tqdm()\n","\n","        for data in train_dataloader:\n","\n","            input , label = data\n","            optimizer.zero_grad()\n","            batch_out = model(input,label)\n","            loss = batch_out['loss']\n","            loss.backward()\n","            optimizer.step()\n","\n","            progress_bar.update()\n","\n","        t_acc , t_loss = train_acc(model)\n","        v_acc , v_loss = test_acc(model)\n","      \n","        progress_bar.set_postfix(epoch=epoch, train_loss = t_loss , val_loss = v_loss,train_acc = t_acc , val_acc = v_acc)\n","\n","        #save models according to accuracy\n","        if (v_acc> max_acc):\n","          print('saving...')\n","          torch.save(model.state_dict(),  descriptor + '.ph')\n","          max_acc = v_acc\n","\n","        history_acc_train.append(t_acc)\n","        history_acc_val.append(v_acc)\n","        history_loss_train.append(t_loss)\n","        history_loss_val.append(v_loss)\n","\n","        progress_bar.close()\n","\n","    #save plots\n","    history_acc = [history_acc_train , history_acc_val]\n","    history_loss = [history_loss_train, history_loss_val]\n","    for step in history_acc:\n","      plt.plot(step)\n","      plt.xlabel('Epochs')\n","      plt.ylabel('Accuracy')\n","\n","    plt.savefig(descriptor + '_ACC.png')  \n","    plt.close()\n","\n","    for step in history_loss:\n","      plt.plot(step)\n","      plt.xlabel('Epochs')\n","      plt.ylabel('Loss')\n","\n","    plt.savefig(descriptor + '_LOSS.png')\n","    plt.close()\n","\n","    #save file with max_acc as a dict\n","    stdout = {descriptor + '_ACC': max_acc}\n","    out_file = open(\"out.json\", \"w\")\n","    json.dump(stdout, out_file)\n","    out_file. close()\n","\n","    \n","      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gBAL9FVejDcl"},"source":["\n","\n","# Load data and process the data\n"]},{"cell_type":"code","metadata":{"id":"oIO1BP2FEHAT"},"source":["# instantiate organized_words dict and save it as pickle file\n","\n","words_organized  = organize_words('/content/drive/MyDrive/NLP/nlp2021-hw1-main/data/train.jsonl')\n","\n","with open('words_organized.pickle', 'wb') as handle:\n","    pickle.dump(words_organized, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oxxAiI7AvHwD"},"source":["\n","\n","##RNN (LSTM) loading module\n","\n"]},{"cell_type":"code","metadata":{"id":"ytbr73vDKmqg"},"source":["class WiCDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, dataset_path: str, sentence2indices):\n","      self.data_store = []\n","      self.init_structures(dataset_path, sentence2indices)\n","\n","    def init_structures(self, dataset_path: str, sentence2indices) -> None:\n","      with open(dataset_path) as f:\n","        for line in f:\n","          #crete a dict from each sample in the train.json\n","          dict_from_line  = eval(line)\n","\n","          #extraxt elements fro keys \n","          start1 = int(dict_from_line['start1'])\n","          start2 = int(dict_from_line['start2'])\n","          end1 = int(dict_from_line['end1'])\n","          end2 = int(dict_from_line['end2'])\n","          sentence1 = dict_from_line['sentence1'].lower()\n","          sentence2 = dict_from_line['sentence2'].lower()\n","          lemma = dict_from_line['lemma']\n","          label = dict_from_line['label']\n","          if (label == 'True'):\n","            label = torch.Tensor([1])\n","            \n","          else:\n","            label = torch.Tensor([0])\n","\n","\n","          # here we could have all the different preprocess functions defined above such as search and replace lemma\n","          # remove numbers or refine sentence, whihc I do not display since they turned out to enworse performances \n","\n","          # SCONCAT\n","          '''\n","\n","          sentence1 = search_replace_word(sentence1,lemma,start1,end1)\n","          sentence2 = search_replace_word(sentence2,lemma,start2,end2)\n","          sentence_concat = sentence1 +' ~ '+sentence2+' ~ '+lemma\n","          sentence_concat = rm_punctuation(sentence_concat)\n","          sentence_concat = rm_words(sentence_concat,words_organized['few_occ'])\n","          sentence_concat = rm_words(sentence_concat,words_organized['most_occ'])\n","          \n","          '''\n","\n","          # TCONCAT\n","\n","          sentence1 = rm_punctuation(sentence1)\n","          sentence2 = rm_punctuation(sentence2)\n","          sentence1 = rm_words(sentence1,words_organized['most_occ'])\n","          sentence2 = rm_words(sentence2,words_organized['most_occ'])\n","          input_tensor1 = sentence2indices(sentence1)\n","          input_tensor2 = sentence2indices(sentence2)\n","          input_tensor = torch.cat((input_tensor1, input_tensor2), dim=0)\n","          self.data_store.append((input_tensor,label))\n","\n","    def __len__(self) -> int:\n","        return len(self.data_store)\n","\n","    def __getitem__(self, idx: int) -> torch.Tensor:\n","        return self.data_store[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70ENDyvbwA4f"},"source":["train_dataset  = WiCDataset('/content/drive/MyDrive/NLP/nlp2021-hw1-main/data/train.jsonl',sentence2indices)\n","test_dataset  = WiCDataset('/content/drive/MyDrive/NLP/nlp2021-hw1-main/data/dev.jsonl',sentence2indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rAi_31ZvwjaW"},"source":["train_dataloader = DataLoader(train_dataset, batch_size=32,collate_fn=rnn_collate_fn)\n","test_dataloader = DataLoader(test_dataset, batch_size=32,collate_fn=rnn_collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bEdKJkzFvU0R"},"source":["\n","\n","## MLP loading module\n","\n"]},{"cell_type":"code","metadata":{"id":"KnXGBHrjvkuF"},"source":["class WiCDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, dataset_path: str, sentence2vector):\n","      self.data_store = []\n","      self.init_structures(dataset_path, sentence2vector)\n","\n","    def init_structures(self, dataset_path: str, sentence2vector) -> None:\n","      with open(dataset_path) as f:\n","        for line in f:\n","          #crete a dict from each sample in the train.json\n","          dict_from_line  = eval(line)\n","\n","          #extraxt elements fro keys \n","          start1 = int(dict_from_line['start1'])\n","          start2 = int(dict_from_line['start2'])\n","          end1 = int(dict_from_line['end1'])\n","          end2 = int(dict_from_line['end2'])\n","          sentence1 = dict_from_line['sentence1'].lower()\n","          sentence2 = dict_from_line['sentence2'].lower()\n","          lemma = dict_from_line['lemma']\n","          label = dict_from_line['label']\n","          if (label == 'True'):\n","            label = torch.Tensor([1])\n","            \n","          else:\n","            label = torch.Tensor([0])\n","\n","          # here we could have all the different preprocess functions defined above such as search and replace lemma\n","          # remove numbers or refine sentence, whihc I do not display since they turned out to enworse performances  \n","\n","\n","          # SCONCAT\n","          '''\n","          sentence_concat = sentence1 +' ~ '+sentence2\n","          sentence_concat = rm_punctuation(sentence_concat)\n","          #sentence_concat = rm_words(sentence_concat,words_organized['few_occ'])\n","          sentence_concat = rm_words(sentence_concat,words_organized['most_occ'])\n","          input_tensor = sentence2vector(sentence_concat)\n","          self.data_store.append((input_tensor,label))\n","          \n","          '''\n","\n","          # TCONCAT\n","          sentence1 = rm_punctuation(sentence1)\n","          sentence2 = rm_punctuation(sentence2)\n","          sentence1 = rm_words(sentence1,words_organized['few_occ'])\n","          sentence2 = rm_words(sentence2,words_organized['few_occ'])\n","          sentence1 = rm_words(sentence1,words_organized['most_occ'])\n","          sentence2 = rm_words(sentence2,words_organized['most_occ'])\n","          input_tensor1 = sentence2vector(sentence1)\n","          input_tensor2 = sentence2vector(sentence2)\n","          input_tensor = torch.cat((input_tensor1, input_tensor2), dim=0)\n","          self.data_store.append((input_tensor,label))\n","          \n","\n","    def __len__(self) -> int:\n","        return len(self.data_store)\n","\n","    def __getitem__(self, idx: int) -> torch.Tensor:\n","        return self.data_store[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DeuwfZCvkuG"},"source":["train_dataset  = WiCDataset('/content/drive/MyDrive/NLP/nlp2021-hw1-main/data/train.jsonl',sentence2vector)\n","test_dataset  = WiCDataset('/content/drive/MyDrive/NLP/nlp2021-hw1-main/data/dev.jsonl',sentence2vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNVdyOlGvkuG"},"source":["train_dataloader = DataLoader(train_dataset, batch_size=32)\n","test_dataloader = DataLoader(test_dataset, batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_93X9q_xjKAK"},"source":["\n","\n","# Models Used for MLP and LSTM experiment\n","\n","### Here I define the different model architectures/structures I used for the experiments:\n","\n","##strct0 \n","\n","####WiCClassifier(\n","####  (lin1): Linear(in_features=300, out_features=N, bias=True)\n","####  (lin2): Linear(in_features=N, out_features=1, bias=True)\n","####  (loss_fn): BCELoss())\n","------------------------------------------------------------------------\n","##strct1 \n","\n","###WiCClassifier(\n","####  (lin1): Linear(in_features=300, out_features=int(n_hidden/2), bias=True)\n","####  (lin2): Linear(in_features=int(n_hidden/2), out_features=int(n_hidden/4), bias=True)\n","####  (lin3): Linear(in_features=int(n_hidden/4), out_features=1, bias=True)\n","####  (loss_fn): BCELoss())\n","------------------------------------------------------------------------\n","##strct2\n","\n","###3WiCClassifier(\n","####  (lin1): Linear(in_features=300, out_features=N, bias=True)\n","####  (lin2): Linear(in_features=N, out_features=N/8, bias=True)\n","####  (lin3): Linear(in_features=N/8, out_features=1, bias=True)\n","####  (loss_fn): BCELoss())\n","------------------------------------------------------------------------\n","##strct3\n","\n","####WiCClassifier(\n","####  (lin1): Linear(in_features=300, out_features=N, bias=True)\n","####  (lin2): Linear(in_features=N, out_features=N/2, bias=True)\n","####  (lin3): Linear(in_features=N/2, out_features=N/16, bias=True)\n","####  (lin4): Linear(in_features=N/16, out_features=1, bias=True)\n","####  (loss_fn): BCELoss())\n","------------------------------------------------------------------------\n","##strct4\n","\n","####WiCClassifier(\n","####  (lin1): Linear(in_features=300, out_features=N, bias=True)\n","####  (lin2): Linear(in_features=N, out_features=N/4, bias=True)\n","####  (lin3): Linear(in_features=N/4, out_features=N/16, bias=True)\n","####  (lin4): Linear(in_features=N/16, out_features=1, bias=True)\n","####  (loss_fn): BCELoss())\n","------------------------------------------------------------------------\n","##strct5\n","\n","####WiCClassifier(\n","####  (lin1): Linear(in_features=300, out_features=N, bias=True)\n","####  (lin2): Linear(in_features=N, out_features=N/2, bias=True)\n","####  (drop1): Dropout(p=0.5, inplace=False)\n","####  (lin3): Linear(in_features=N/2, out_features=N/16, bias=True)\n","####  (drop2): Dropout(p=0.5, inplace=False)\n","####  (lin4): Linear(in_features=N/16, out_features=1, bias=True)\n","####  (loss_fn): BCELoss())\n","\n","### (this architectures have been used both for MLP and LSTM )\n","------------------------------------------------------------------------\n","\n","##strct6\n","\n","####WiCClassifier(\n","####  (embedding): Embedding(400001, 300)\n","####  (rnn1): LSTM(300, N, batch_first=True)\n","####  (lin1): Linear(in_features=N, out_features=2*N, bias=True)\n","####  (lin2): Linear(in_features=2*N, out_features=2, bias=True)\n","####  (loss_fn): CrossEntropyLoss())\n","\n","### (this last one has been used only for LSTM; it is the architecture which performed better)\n","\n"," \n","\n"]},{"cell_type":"code","metadata":{"id":"aTZpGZZVDl_S"},"source":["# define the LSTM model\n","\n","class WiCClassifier(nn.Module):\n","\n","    def __init__(\n","        self,\n","        vectors_store: torch.Tensor,\n","        n_hidden: int\n","    ) -> None:\n","        super().__init__()\n","\n","        # embedding layer\n","        self.embedding = torch.nn.Embedding.from_pretrained(vectors_store)\n","\n","        # recurrent layer\n","        self.rnn1 = torch.nn.LSTM(input_size=vectors_store.size(1), hidden_size=n_hidden, num_layers=1, batch_first=True)\n","        self.bn1 = torch.nn.BatchNorm1d(n_hidden)\n","        self.drop1 = torch.nn.Dropout(0.5)\n","\n","        # classification head\n","        self.lin1 = torch.nn.Linear(n_hidden, int(n_hidden*2))\n","        self.bn2 = torch.nn.BatchNorm1d(int(n_hidden*2))\n","        self.drop2 = torch.nn.Dropout(0.5)\n","\n","        self.lin2 = torch.nn.Linear(int(n_hidden*2), 2)\n","\n","        # criterion\n","        self.loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    def forward(\n","        self, \n","        X: torch.Tensor, \n","        X_length: torch.Tensor, \n","        y: Optional[torch.Tensor] = None\n","    ) -> Dict[str, torch.Tensor]:\n","\n","        # embedding words from indices\n","        embedding_out = self.embedding(X)\n","        # recurrent encoding\n","        recurrent_out = self.rnn1(embedding_out)[0]\n","\n","        batch_size, seq_len, hidden_size = recurrent_out.shape\n","\n","        # here I used an adapted approach of the solution proposed during the lectures\n","        flattened_out = recurrent_out.reshape(-1, hidden_size)\n","        last_word_relative_indices = X_length - 1\n","        sequences_offsets = torch.arange(batch_size)*seq_len\n","        summary_vectors_indices = sequences_offsets + last_word_relative_indices\n","        summary_vectors = flattened_out[summary_vectors_indices]\n","  \n","        out = self.bn1(summary_vectors)\n","        out = self.drop1(out)\n","\n","        out = self.lin1(out)\n","        out = self.bn2(out)\n","        out = torch.relu(out)\n","        out = self.drop2(out)\n"," \n","\n","        out = self.lin2(out)\n","\n","        pred = torch.softmax(out,-1)\n","        \n","        result = {'logits': out, 'pred': pred}\n","\n","        if y is not None:\n","          loss = self.loss(out, y)\n","          result['loss'] = loss\n","\n","        return result\n","\n","    def loss(self, out, y):\n","        return self.loss_fn(out, y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDJ0lRR1v_SN"},"source":["# define the MLP model\n","\n","class WiCClassifier(nn.Module):\n","\n","    def __init__( self,  n_features: int, n_hidden: int ):\n","        super().__init__()\n","\n","        # embedding layer\n","\n","        # classification head\n","        self.lin1 = torch.nn.Linear(n_features,  n_hidden)\n","\n","        self.lin2 = torch.nn.Linear( n_hidden,int(n_hidden/4))\n","        self.drop1 = torch.nn.Dropout(0.5)\n","\n","        self.lin3 = torch.nn.Linear( int(n_hidden/4), int(n_hidden/16))\n","        self.drop2 = torch.nn.Dropout(0.5)\n","\n","        self.lin4 = torch.nn.Linear( n_hidden,1)\n","\n","\n","        # criterion\n","        self.loss_fn = torch.nn.BCELoss()\n","\n","    def forward( self, x: torch.Tensor,  y: Optional[torch.Tensor] = None ) -> Dict[str, torch.Tensor]:\n","\n","        out = self.lin1(x)\n","        out = torch.relu(out)\n","\n","        out = self.lin2(out)\n","        out = torch.relu(out)\n","        out = self.drop1(out)\n","\n","        out = self.lin3(out)\n","        out = torch.relu(out)\n","        out = self.drop2(out)\n","\n","        out = self.lin4(out)\n","\n","        out = torch.sigmoid(out)\n","        \n","        result = {'pred': out}\n","\n","        if y is not None:\n","\n","          loss = self.loss(out, y)\n","          result['loss'] = loss\n","\n","        return result\n","\n","    def loss(self, out, y):\n","        return self.loss_fn(out, y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHY9MCMDjPXc"},"source":["\n","\n","# Training loop and testing loop \n"]},{"cell_type":"code","metadata":{"id":"7vPm3zHGtGDF"},"source":["# build the model we used ADAM or SGG optimizer\n","\n","model = WiCClassifier(vectors_store, 50)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","model.state_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4VDpPDH2EDO"},"source":["# train and evaluate the model\n","\n","training_loop(model, optimizer,epochs = 70)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ybc2jFkSwez_"},"source":["\n","\n","# Experiment\n","\n","#### I built an experiment to find out a good combination of hyperparameters and to test different architectures both for LSTM and MLP.\n"]},{"cell_type":"code","metadata":{"id":"l6WjjkzU3BRD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619386352253,"user_tz":-120,"elapsed":76895,"user":{"displayName":"lorenzo mandelli","photoUrl":"","userId":"01769209783218279689"}},"outputId":"cdcd9548-ae07-4be5-c056-4494d9f8fdfa"},"source":["#Go in the working directory\n","\n","%cd '/content/drive/MyDrive/NLP/Experiment/model_TCONCAT/model_5'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/NLP/Experiment/model_Adam/model_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zRnEnB6sqq6_"},"source":["#Experiment ( hype_2 changes depending on the Optimizer, here we use SGD )\n","\n","arch = '5strct'\n","base_path = '/content/drive/MyDrive/NLP/Experiment/model_TCONCAT/model_5'\n","for hype_1 in range(32,160,8):\n","  hype_2 = 0.01\n","  for i in range(0,5):\n","\n","    os.chdir(base_path)\n","    hype_2 += 0.02\n","    \n","    descriptor = arch + '_' + str(hype_1) + '_' +str(round(hype_2,4)) \n","\n","    #create directory and go into it\n","\n","    os.makedirs(descriptor)\n","    os.chdir(descriptor)\n","\n","    model = WiCClassifier(n_features = 300, n_hidden = hype_1 )\n","    optimizer = torch.optim.SGD(model.parameters(), lr=hype_2)\n","    training_loop(model, optimizer,epochs = 80, descriptor = descriptor)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O_EuEyX0ydXs"},"source":["\n","\n","\n","# Tester\n","#### used to explore quickly the results obtained from the experiment  the hyperparamters and loops descriptors must coincide between experiment and tester in order to guarantee a complete exploration of the results\n"]},{"cell_type":"code","metadata":{"id":"PE0m8meJIuEF"},"source":["# use the same hype_1 and hype_2  experiment settings \n","arch = '5strct'\n","base_path = '/content/drive/MyDrive/NLP/Experiment/model_TCONCAT/model_5'\n","max_acc = 0 \n","\n","for hype_1 in range(16,160,8):\n","  hype_2 = 0.01\n","  for i in range(0,5):\n","    os.chdir(base_path)\n","    hype_2 += 0.02\n","    \n","    descriptor = arch + '_' + str(hype_1) + '_' +str(round(hype_2,4)) \n","    try:\n","\n","      os.chdir(descriptor)\n","      with open('out.json') as f:\n","        for line in f:\n","          line = eval(line)\n","          for k,v in line.items():\n","            #print(k,v)\n","            if (v > max_acc): \n","              best_model = k\n","              max_acc = v\n","              print(best_model,max_acc)\n","    except:\n","      continue\n","        \n"],"execution_count":null,"outputs":[]}]}